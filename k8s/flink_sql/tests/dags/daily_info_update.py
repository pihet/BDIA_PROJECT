"""
ì¼ì¼ ì •ë³´ ê°±ì‹  DAG (arrears_info, missing_person_info)
- ë§¤ì¼ 00:00ì— ì‹¤í–‰
- Flink SQLë¡œ RDS â†’ Kafka ì „ì†¡
"""

from airflow.decorators import dag, task
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

# í•œêµ­ ì‹œê°„ëŒ€
KST = ZoneInfo("Asia/Seoul")


@dag(
    dag_id='daily_info_update',
    description='ì²´ë‚©/ì‹¤ì¢…ì ì •ë³´ ì¼ì¼ ê°±ì‹ ',
    schedule='0 0 * * *',  # ë§¤ì¼ 00:00
    start_date=datetime(2025, 1, 1, tzinfo=KST),
    catchup=False,
    tags=['flink', 'batch', 'daily'],
    default_args={
        'owner': 'airflow',
        'depends_on_past': False,
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 2,
        'retry_delay': timedelta(minutes=5),
    }
)
def daily_info_update_dag():
    """
    ì²´ë‚©/ì‹¤ì¢…ì ì •ë³´ ì¼ì¼ ê°±ì‹  DAG
    """
    
    @task
    def run_daily_info_update():
        """
        Flink SQL ì‹¤í–‰
        """
        import subprocess
        
        print("ğŸ”„ ì¼ì¼ ì •ë³´ ê°±ì‹  ì‹œì‘...")
        
        # Flink SQL ì‹¤í–‰ ëª…ë ¹ì–´
        cmd = """
        kubectl exec -i -n flink flink-sql-client-<POD_NAME> -- \
        /opt/flink/bin/sql-client.sh embedded \
        -f /opt/airflow/dags/flink_sql/02_daily_info_update.sql
        """
        
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… ì¼ì¼ ì •ë³´ ê°±ì‹  ì™„ë£Œ")
            print(result.stdout)
        else:
            print("âŒ ì¼ì¼ ì •ë³´ ê°±ì‹  ì‹¤íŒ¨")
            print(result.stderr)
            raise Exception(f"Flink ì‘ì—… ì‹¤íŒ¨: {result.stderr}")
        
        return result.returncode
    
    
    # Task ì‹¤í–‰
    run_daily_info_update()


# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
dag_instance = daily_info_update_dag()
